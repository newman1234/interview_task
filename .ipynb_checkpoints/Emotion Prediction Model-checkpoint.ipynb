{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data number: 3001.\n",
      "\n",
      "No score for some rows. Delete from all_text.\n",
      "[' ', '但人真的很多\\n']\n",
      "[' ', '问我们价格如何啊\\n']\n",
      "[' ', '钱包瘦身不少\\n']\n",
      "[' ', '虽然更迭了不少品牌\\n']\n",
      "[' ', '现在不去百货店了\\n']\n",
      "[' ', '中午白领吃饭什么的人很多的\\n']\n",
      "[' ', '还早杀到上海奥特莱斯\\n']\n",
      "[' ', '都还不错\\n']\n",
      "[' ', '小吃\\n']\n",
      "[' ', '就是逛逛\\n']\n",
      "[' ', '记得他结婚时也选择这个品牌\\n']\n",
      "[' ', '现在国金和世贸开出之后就比较少光顾了\\n']\n",
      "[' ', '经常和闺蜜去\\n']\n",
      "[' ', '没法经常去\\n']\n",
      "\n",
      "Preprocessing: score to int, text truncate \\n.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ' '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-53f43f71f184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nPreprocessing: score to int, text truncate \\\\n.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ' '"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import random\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "random.seed(12)\n",
    "%matplotlib inline\n",
    "\n",
    "f = open('Ch_trainfile_Sentiment_3000.txt')\n",
    "all_text = [line.split('\\t') for line in f.readlines()] # use tab to split\n",
    "all_text_processed = []\n",
    "print('Original data number: {}.'.format(len(all_text)))\n",
    "\n",
    "print('\\nNo score for some rows. Delete from all_text.')\n",
    "for line in all_text: # no score for some rows\n",
    "    if len(line[0])<2:\n",
    "        print(line)\n",
    "        \n",
    "print('\\nPreprocessing: score to int, text truncate \\\\n.')\n",
    "for score, text in all_text:\n",
    "    print(text)\n",
    "    score = int(score)\n",
    "    text = text.split('\\n')[0]\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c,'')\n",
    "    text = text.replace(' ','')\n",
    "    text = text.replace('、','')\n",
    "    all_text_processed.append((score, text))\n",
    "print('Processed data number: {}.'.format(len(all_text_processed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation for Chinese words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_set = set()\n",
    "all_text_seg = []\n",
    "\n",
    "i = 0\n",
    "N = 100\n",
    "FULL_MODE = False\n",
    "print('Print first {} cut results.'.format(N))\n",
    "for score, text in all_text_processed:\n",
    "    word_list = list(jieba.cut(text, cut_all=FULL_MODE))\n",
    "    if i < N:\n",
    "        print(i, '/'.join(word_list))\n",
    "    i += 1\n",
    "    all_text_seg.append((score, word_list))\n",
    "    word_set |= set(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #build K fold randomly\n",
    "# K = 5\n",
    "# def chunk(seq, num):\n",
    "#     avg = len(seq) / float(num)\n",
    "#     out = []\n",
    "#     last = 0.0\n",
    "#     while last < len(seq):\n",
    "#         out.append(seq[int(last):int(last + avg)])\n",
    "#         last += avg\n",
    "#     return out\n",
    "\n",
    "# random.shuffle(all_text_seg)\n",
    "# data_fold = list(chunk(all_text_seg, K))\n",
    "# for i, fold in enumerate(data_fold):\n",
    "#     print('Total length in fold {}: {}.'.format(i, len(fold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buid feature set, Transform cut text to feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import RidgeClassifier\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.linear_model import Perceptron\n",
    "# from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "# from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.neighbors import NearestCentroid\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Bag of Words Binary model for SVM and better model\n",
    "# mean_scores = []\n",
    "# bestSVM = None\n",
    "# bestAcc = 0.0\n",
    "# is_feature_all = True\n",
    "\n",
    "# GridSearchCV()\n",
    "\n",
    "# scores = {}\n",
    "# clf = None\n",
    "# for i in range(K):\n",
    "#     #train\n",
    "#     train = []\n",
    "#     for j in range(K):\n",
    "#         if j != i:\n",
    "#             train += data_fold[j]\n",
    "#     if is_feature_all:\n",
    "#         # Option 1: use the whole datasets as feature\n",
    "#         feature = list(word_set)\n",
    "#         # Option 2: only use the training datasets as feature\n",
    "#         N_train = len(train)\n",
    "#         feature = list(set([w for s, t in train for w in t]))\n",
    "#         # print('Feature number for {} as validation: {}.'.format(i, len(feature)))\n",
    "\n",
    "#     X_train = []\n",
    "#     y_train = []\n",
    "#     for s, t in train:\n",
    "#         word_vector = []\n",
    "#         y_train.append(s)\n",
    "#         for w in feature:\n",
    "#             if w in t:\n",
    "#                 word_vector.append(1)\n",
    "#             else:\n",
    "#                 word_vector.append(0)\n",
    "#         X_train.append(word_vector)\n",
    "\n",
    "#     X_train = np.array(X_train).astype('float64')\n",
    "#     y_train = np.array(y_train).astype('float64')\n",
    "\n",
    "#     #validation\n",
    "#     val = data_fold[i]\n",
    "#     X_val = []\n",
    "#     y_val = []\n",
    "#     for s, t in val:\n",
    "#         word_vector = []\n",
    "#         y_val.append(s)\n",
    "#         for w in feature:\n",
    "#             if w in t:\n",
    "#                 word_vector.append(1)\n",
    "#             else:\n",
    "#                 word_vector.append(0)\n",
    "#         X_val.append(word_vector)\n",
    "\n",
    "#     X_val = np.array(X_val).astype('float64')\n",
    "#     y_val = np.array(y_val).astype('float64')\n",
    "\n",
    "#     # Classification\n",
    "#     clf = LinearSVC(C=C_param[0])\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X_val)\n",
    "#     scores.append(accuracy_score(y_val, y_pred))\n",
    "# mean_scores.append(np.mean(scores))\n",
    "# if mean_scores[-1] > bestAcc:\n",
    "#     bestAcc = mean_scores[-1]\n",
    "#     bestSVM = clf\n",
    "\n",
    "\n",
    "# plt.plot(C_param, mean_scores)\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('Average validation score')\n",
    "# plt.title('Validation Accuracy for Linear SVM Model')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
